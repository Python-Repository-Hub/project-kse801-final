{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import dgl\n",
    "from dgl.data.utils import load_graphs\n",
    "import torch\n",
    "import torch.nn as nn"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "data = load_graphs('../data/gsi2.bin')\n",
    "graph = data[0][0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# My Module"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "encoder = nn.Sequential(\n",
    "    nn.Linear(3, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 8),\n",
    "    nn.ReLU()\n",
    ")\n",
    "nn.init.xavier_uniform_(encoder.weight)\n",
    "nn.init.constant_(encoder.bias, 0.0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.linear1 = nn.Linear(3, 64)\n",
    "        self.linear2 = nn.Linear(64, 64)\n",
    "        self.linear3 = nn.Linear(64, 8)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.relu3 = nn.ReLU()\n",
    "        torch.nn.init.xavier_uniform_(self.linear1.weight)\n",
    "        torch.nn.init.constant_(self.linear1.bias, 0.0)\n",
    "        torch.nn.init.xavier_uniform_(self.linear2.weight)\n",
    "        torch.nn.init.constant_(self.linear2.bias, 0.0)\n",
    "        torch.nn.init.xavier_uniform_(self.linear3.weight)\n",
    "        torch.nn.init.constant_(self.linear3.bias, 0.0)\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.linear3(x)\n",
    "        x = self.relu3(x)\n",
    "        return x\n",
    "        "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Ori Module"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn.init as init\n",
    "\n",
    "TORCH_ACTIVATION_LIST = ['ReLU',\n",
    "                         'Sigmoid',\n",
    "                         'SELU',\n",
    "                         'LeakyReLU',\n",
    "                         'Softplus',\n",
    "                         'Tanh']\n",
    "\n",
    "ACTIVATION_LIST = ['Mish', 'Swish', 'Absolute', 'IdentityClip', None]\n",
    "\n",
    "\n",
    "class Mish(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * (torch.tanh(F.softplus(x)))\n",
    "\n",
    "\n",
    "class Swish(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * F.sigmoid(x)\n",
    "\n",
    "\n",
    "class Absolute(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.abs(x)\n",
    "\n",
    "\n",
    "class IdentityClip(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.clamp(0.0, 1.0)\n",
    "\n",
    "\n",
    "def get_nn_activation(activation: 'str'):\n",
    "    if not activation in TORCH_ACTIVATION_LIST + ACTIVATION_LIST:\n",
    "        raise RuntimeError(\"Not implemented activation function!\")\n",
    "\n",
    "    if activation in TORCH_ACTIVATION_LIST:\n",
    "        act = getattr(torch.nn, activation)()\n",
    "\n",
    "    if activation in ACTIVATION_LIST:\n",
    "        if activation == 'Mish':\n",
    "            act = Mish()\n",
    "        elif activation == 'Swish':\n",
    "            act = Swish()\n",
    "        elif activation == 'Absolute':\n",
    "            act = Absolute()\n",
    "        elif activation == 'IdentityClip':\n",
    "            act = IdentityClip()\n",
    "        elif activation is None:\n",
    "            act = nn.Identity()\n",
    "\n",
    "    return act\n",
    "\n",
    "\n",
    "class NoisyLinear(nn.Linear):\n",
    "    # Adapted from the original source\n",
    "    # https://github.com/Kaixhin/NoisyNet-A3C/blob/master/model.py\n",
    "\n",
    "    def __init__(self, in_features, out_features, sigma_init=0.017):\n",
    "        super(NoisyLinear, self).__init__(in_features, out_features, bias=True)  # TODO: Adapt for no bias\n",
    "        # µ^w and µ^b reuse self.weight and self.bias\n",
    "        self.sigma_init = sigma_init\n",
    "        self.sigma_weight = Parameter(torch.Tensor(out_features, in_features))  # σ^w\n",
    "        self.sigma_bias = Parameter(torch.Tensor(out_features))  # σ^b\n",
    "        self.register_buffer('epsilon_weight', torch.zeros(out_features, in_features))\n",
    "        self.register_buffer('epsilon_bias', torch.zeros(out_features))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        if hasattr(self, 'sigma_weight'):  # Only init after all params added (otherwise super().__init__() fails)\n",
    "            init.uniform_(self.weight, -math.sqrt(3 / self.in_features), math.sqrt(3 / self.in_features))\n",
    "            init.uniform_(self.bias, -math.sqrt(3 / self.in_features), math.sqrt(3 / self.in_features))\n",
    "            init.constant_(self.sigma_weight, self.sigma_init)\n",
    "            init.constant_(self.sigma_bias, self.sigma_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.linear(x, self.weight + self.sigma_weight * self.epsilon_weight,\n",
    "                        self.bias + self.sigma_bias * self.epsilon_bias)\n",
    "\n",
    "    def sample_noise(self):\n",
    "        self.epsilon_weight = torch.randn(self.out_features, self.in_features)\n",
    "        self.epsilon_bias = torch.randn(self.out_features)\n",
    "\n",
    "    def remove_noise(self):\n",
    "        self.epsilon_weight = torch.zeros(self.out_features, self.in_features)\n",
    "        self.epsilon_bias = torch.zeros(self.out_features)\n",
    "\n",
    "\n",
    "class LinearModule(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 activation: 'str',\n",
    "                 norm: 'str' = None,\n",
    "                 dropout_p: 'float' = 0.0,\n",
    "                 weight_init: 'str' = None,\n",
    "                 use_noisy: bool = False,\n",
    "                 use_residual: bool = False,\n",
    "                 **linear_kwargs):\n",
    "        super(LinearModule, self).__init__()\n",
    "\n",
    "        if linear_kwargs['in_features'] == linear_kwargs['out_features'] and use_residual:\n",
    "            self.use_residual = True\n",
    "        else:\n",
    "            self.use_residual = False\n",
    "\n",
    "        # layers\n",
    "        if use_noisy:\n",
    "            linear_layer = NoisyLinear(**linear_kwargs)\n",
    "        else:\n",
    "            linear_layer = torch.nn.Linear(**linear_kwargs)\n",
    "\n",
    "        self.linear_layer = linear_layer\n",
    "        if dropout_p > 0.0:\n",
    "            self.dropout_layer = torch.nn.Dropout(dropout_p)\n",
    "        else:\n",
    "            self.dropout_layer = torch.nn.Identity()\n",
    "        self.activation_layer = get_nn_activation(activation)\n",
    "\n",
    "        self.weight_init = weight_init\n",
    "        self.activation = activation\n",
    "        self.norm = norm\n",
    "\n",
    "        # apply weight initialization methods\n",
    "        self.apply_weight_init(self.linear_layer, self.weight_init)\n",
    "\n",
    "        if norm == 'batch':\n",
    "            self.norm_layer = torch.nn.BatchNorm1d(self.linear_layer.out_features)\n",
    "        elif norm == 'layer':\n",
    "            self.norm_layer = torch.nn.LayerNorm(self.linear_layer.out_features)\n",
    "        elif norm == 'spectral':\n",
    "            self.linear_layer = torch.nn.utils.spectral_norm(self.linear_layer)\n",
    "            self.norm_layer = torch.nn.Identity()\n",
    "        elif norm is None:\n",
    "            self.norm_layer = torch.nn.Identity()\n",
    "        else:\n",
    "            raise RuntimeError(\"Not implemented normalization function!\")\n",
    "\n",
    "    def apply_weight_init(self, tensor, weight_init=None):\n",
    "        if weight_init is None:\n",
    "            pass  # do not apply weight init\n",
    "        elif weight_init == \"normal\":\n",
    "            torch.nn.init.normal_(tensor.weight, std=0.3)\n",
    "            torch.nn.init.constant_(tensor.bias, 0.0)\n",
    "        elif weight_init == \"kaiming_normal\":\n",
    "            if self.activation in ['sigmoid', 'tanh', 'relu', 'leaky_relu']:\n",
    "                torch.nn.init.kaiming_normal_(tensor.weight, nonlinearity=self.activation)\n",
    "                torch.nn.init.constant_(tensor.bias, 0.0)\n",
    "            else:\n",
    "                pass\n",
    "        elif weight_init == \"xavier\":\n",
    "            torch.nn.init.xavier_uniform_(tensor.weight)\n",
    "            torch.nn.init.constant_(tensor.bias, 0.0)\n",
    "        else:\n",
    "            raise NotImplementedError(\"MLP initializer {} is not supported\".format(weight_init))\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_residual:\n",
    "            input_x = x\n",
    "\n",
    "        x = self.linear_layer(x)\n",
    "        x = self.norm_layer(x)\n",
    "        x = self.activation_layer(x)\n",
    "        x = self.dropout_layer(x)\n",
    "\n",
    "        if self.use_residual:\n",
    "            x = input_x + x\n",
    "        return x\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MultiLayerPerceptron(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_dimension: int,\n",
    "                 output_dimension: int,\n",
    "                 num_neurons: list = [64, 64],\n",
    "                 activation='Mish',\n",
    "                 out_activation='Mish',\n",
    "                 normalization=None,\n",
    "                 weight_init='xavier',\n",
    "                 dropout_probability=0.0,\n",
    "                 use_noisy=False):\n",
    "\n",
    "        super(MultiLayerPerceptron, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dimension\n",
    "        self.output_dim = output_dimension\n",
    "        self.num_neurons = num_neurons\n",
    "        self.use_noisy = use_noisy\n",
    "\n",
    "        _list_norm = self.check_input_spec(normalization)\n",
    "        _input_norm = True if _list_norm and len(normalization) == 1 else False\n",
    "        _list_act = self.check_input_spec(activation)\n",
    "        _list_drop_p = self.check_input_spec(dropout_probability)\n",
    "\n",
    "        input_dims = [input_dimension] + num_neurons\n",
    "        output_dims = num_neurons + [output_dimension]\n",
    "\n",
    "        # Input -> the last hidden layer\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i, (in_dim, out_dim) in enumerate(zip(input_dims[:-1], output_dims[:-1])):\n",
    "            norm = normalization[i] if _list_norm else normalization\n",
    "            norm = None if _input_norm and i != 0 else norm\n",
    "            act = activation[i] if _list_act else activation\n",
    "            drop_p = dropout_probability[i] if _list_drop_p else dropout_probability\n",
    "\n",
    "            linear_module = LinearModule(in_features=in_dim, out_features=out_dim, activation=act,\n",
    "                                         norm=norm, dropout_p=drop_p, weight_init=weight_init, use_noisy=use_noisy)\n",
    "            self.layers.append(linear_module)\n",
    "\n",
    "        output_layer = LinearModule(in_features=input_dims[-1], out_features=output_dims[-1],\n",
    "                                    activation=out_activation,\n",
    "                                    norm=None, dropout_p=0.0, weight_init=weight_init, use_noisy=use_noisy)\n",
    "        self.layers.append(output_layer)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def check_input_spec(self, input_spec):\n",
    "        if isinstance(input_spec, list):\n",
    "            # output layer will not be normalized\n",
    "            assert len(input_spec) == len(self.num_neurons) + 1, \"the length of input_spec list should \" \\\n",
    "                                                                 \"match with the number of hidden layers + 1\"\n",
    "            _list_type = True\n",
    "        else:\n",
    "            _list_type = False\n",
    "\n",
    "        return _list_type"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "model = MultiLayerPerceptron(3, 8, [64, 64], 'ReLU', 'ReLU')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "result1 = encoder(graph.ndata['feat'])\n",
    "result2 = model(graph.ndata['feat'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "model2 = Encoder()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "result3 = model2(graph.ndata['feat'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "print(result1)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 1.7874e-01, 0.0000e+00, 3.8676e-01,\n",
      "         5.8329e-02, 0.0000e+00],\n",
      "        [7.8709e-02, 9.4251e-02, 2.8755e-02, 0.0000e+00, 0.0000e+00, 8.0551e-02,\n",
      "         1.2337e-01, 0.0000e+00],\n",
      "        [2.7956e-02, 3.5818e-02, 5.9430e-02, 1.0442e-01, 5.7396e-02, 2.2366e-01,\n",
      "         2.5687e-01, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 1.5778e-01, 0.0000e+00, 1.9637e-01,\n",
      "         5.4326e-02, 0.0000e+00],\n",
      "        [7.4405e-02, 1.0214e-01, 4.9250e-02, 1.7544e-02, 0.0000e+00, 9.8163e-02,\n",
      "         1.6029e-01, 0.0000e+00],\n",
      "        [1.0880e-02, 0.0000e+00, 0.0000e+00, 1.2912e-01, 1.9527e-02, 1.7270e-01,\n",
      "         8.1722e-02, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0098e-01, 6.8932e-02, 2.1995e-01,\n",
      "         2.5112e-02, 0.0000e+00],\n",
      "        [0.0000e+00, 6.7834e-02, 2.9889e-02, 0.0000e+00, 0.0000e+00, 1.5372e-01,\n",
      "         2.4137e-03, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 4.8242e-02, 4.6354e-02, 0.0000e+00, 1.1302e-01,\n",
      "         3.9069e-02, 2.8882e-02],\n",
      "        [1.6455e-02, 7.8588e-02, 3.5148e-02, 0.0000e+00, 0.0000e+00, 1.0040e-01,\n",
      "         3.7868e-02, 0.0000e+00],\n",
      "        [8.7741e-02, 8.3709e-02, 3.7368e-02, 0.0000e+00, 0.0000e+00, 1.0599e-01,\n",
      "         1.4811e-01, 0.0000e+00],\n",
      "        [0.0000e+00, 9.0674e-03, 0.0000e+00, 1.4262e-01, 2.2617e-02, 3.1522e-01,\n",
      "         4.4532e-02, 0.0000e+00],\n",
      "        [0.0000e+00, 1.8361e-02, 8.2415e-03, 9.7585e-02, 0.0000e+00, 2.4604e-01,\n",
      "         9.3265e-03, 0.0000e+00],\n",
      "        [1.9623e-02, 0.0000e+00, 0.0000e+00, 1.2538e-01, 4.1916e-02, 2.1544e-01,\n",
      "         1.7113e-01, 0.0000e+00],\n",
      "        [6.3942e-02, 9.3274e-02, 7.2725e-02, 6.4033e-02, 0.0000e+00, 1.3547e-01,\n",
      "         1.9284e-01, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1963e-01, 0.0000e+00, 1.5765e-01,\n",
      "         5.9011e-02, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1185e-01, 0.0000e+00, 2.1102e-01,\n",
      "         5.6784e-02, 0.0000e+00],\n",
      "        [1.1214e-02, 1.8075e-02, 1.4370e-02, 3.5931e-02, 0.0000e+00, 1.1772e-01,\n",
      "         7.9919e-02, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 6.0396e-03, 1.0497e-01, 4.9739e-03, 1.4313e-01,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.3498e-02, 4.2629e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 9.6041e-02,\n",
      "         6.7791e-02, 0.0000e+00],\n",
      "        [1.1359e-02, 0.0000e+00, 0.0000e+00, 6.7672e-02, 0.0000e+00, 1.8065e-01,\n",
      "         6.6801e-02, 6.5064e-03],\n",
      "        [0.0000e+00, 2.3881e-02, 3.7724e-02, 9.1715e-03, 0.0000e+00, 9.4687e-02,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 7.6324e-02, 3.4863e-02, 1.2526e-01,\n",
      "         0.0000e+00, 2.1317e-02],\n",
      "        [5.0896e-02, 6.4092e-02, 2.1076e-02, 0.0000e+00, 0.0000e+00, 9.3155e-02,\n",
      "         7.4022e-02, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 9.8295e-02, 0.0000e+00, 1.9266e-01,\n",
      "         6.5443e-02, 0.0000e+00],\n",
      "        [1.8150e-02, 0.0000e+00, 0.0000e+00, 8.7444e-02, 2.7926e-02, 1.8107e-01,\n",
      "         1.0081e-01, 1.9487e-03],\n",
      "        [0.0000e+00, 2.4969e-02, 2.5182e-02, 5.4294e-02, 0.0000e+00, 1.5845e-01,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.9740e-03, 0.0000e+00, 1.0548e-01, 5.0345e-02, 2.0102e-01,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [2.2712e-02, 7.5681e-02, 3.1639e-03, 2.3343e-02, 0.0000e+00, 7.9603e-02,\n",
      "         9.6926e-02, 0.0000e+00],\n",
      "        [3.9696e-02, 2.6018e-02, 4.1108e-02, 7.5456e-02, 3.1883e-02, 1.7569e-01,\n",
      "         1.5491e-01, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3102e-01, 0.0000e+00, 2.4589e-01,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [3.4127e-04, 0.0000e+00, 0.0000e+00, 4.0292e-02, 0.0000e+00, 1.4583e-01,\n",
      "         5.3742e-02, 3.9334e-02],\n",
      "        [9.6167e-04, 5.8954e-02, 2.7304e-02, 0.0000e+00, 0.0000e+00, 1.0119e-01,\n",
      "         9.0950e-03, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 8.1558e-02, 1.4530e-02, 1.8166e-01,\n",
      "         3.8381e-02, 0.0000e+00],\n",
      "        [0.0000e+00, 8.3271e-03, 0.0000e+00, 1.0920e-02, 0.0000e+00, 9.6294e-02,\n",
      "         5.8988e-02, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 4.1717e-02, 5.8752e-02, 0.0000e+00, 7.2995e-02,\n",
      "         0.0000e+00, 7.9547e-03],\n",
      "        [4.8349e-02, 6.4007e-02, 4.2202e-02, 4.1843e-02, 0.0000e+00, 1.0585e-01,\n",
      "         1.2501e-01, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 3.3601e-02, 2.5033e-03, 0.0000e+00, 8.9392e-02,\n",
      "         2.1232e-02, 0.0000e+00],\n",
      "        [1.3313e-02, 0.0000e+00, 1.1279e-02, 5.7210e-02, 4.2849e-03, 1.2893e-01,\n",
      "         3.6284e-02, 4.4152e-02],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 9.4870e-02, 0.0000e+00, 1.7366e-01,\n",
      "         7.0769e-02, 0.0000e+00],\n",
      "        [4.6207e-02, 7.9857e-02, 1.2247e-02, 2.9424e-03, 0.0000e+00, 9.8986e-02,\n",
      "         1.2057e-01, 0.0000e+00],\n",
      "        [1.3710e-02, 0.0000e+00, 0.0000e+00, 7.9884e-02, 1.2718e-02, 1.6225e-01,\n",
      "         7.6418e-02, 0.0000e+00],\n",
      "        [9.1134e-02, 8.5770e-02, 3.6097e-02, 0.0000e+00, 0.0000e+00, 9.7475e-02,\n",
      "         1.3768e-01, 0.0000e+00],\n",
      "        [4.7594e-02, 8.3797e-02, 3.4106e-02, 0.0000e+00, 0.0000e+00, 9.2951e-02,\n",
      "         7.9609e-02, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3389e-01, 0.0000e+00, 1.7565e-01,\n",
      "         5.3752e-02, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4028e-01, 0.0000e+00, 2.0110e-01,\n",
      "         5.3426e-02, 0.0000e+00],\n",
      "        [0.0000e+00, 6.3846e-02, 2.7593e-02, 0.0000e+00, 0.0000e+00, 1.1256e-01,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 3.4083e-02, 2.1395e-02, 0.0000e+00, 0.0000e+00, 1.0074e-01,\n",
      "         2.6107e-02, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 7.8307e-02, 1.0100e-02, 1.7434e-01,\n",
      "         5.3038e-02, 7.3577e-03]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "print(result2)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0.1100, 0.0000, 0.0387, 0.0735, 0.0000, 0.1325, 0.0732, 0.1426],\n",
      "        [0.3317, 0.4233, 0.0000, 0.0000, 0.0000, 0.2742, 0.0286, 0.0000],\n",
      "        [0.2384, 0.0000, 0.3095, 0.5176, 0.0000, 0.0041, 0.2575, 0.0000],\n",
      "        [0.0340, 0.2966, 0.1094, 0.2645, 0.0000, 0.3023, 0.1576, 0.1016],\n",
      "        [0.2484, 0.0000, 0.1130, 0.3170, 0.0000, 0.1630, 0.0157, 0.0000],\n",
      "        [0.0000, 0.2036, 0.1824, 0.2717, 0.0000, 0.1807, 0.3817, 0.0000],\n",
      "        [0.0138, 0.0320, 0.0448, 0.1571, 0.0000, 0.1795, 0.0000, 0.1605],\n",
      "        [0.1939, 0.1718, 0.1144, 0.0000, 0.0000, 0.2023, 0.0546, 0.0000],\n",
      "        [0.1113, 0.1772, 0.0090, 0.0792, 0.0060, 0.0597, 0.0000, 0.0000],\n",
      "        [0.2256, 0.3170, 0.0000, 0.0000, 0.0000, 0.1989, 0.0251, 0.0000],\n",
      "        [0.2320, 0.1986, 0.0536, 0.0833, 0.0000, 0.2460, 0.0000, 0.0000],\n",
      "        [0.0463, 0.0000, 0.0521, 0.1150, 0.0000, 0.1174, 0.0000, 0.1539],\n",
      "        [0.1663, 0.0151, 0.1143, 0.0000, 0.0000, 0.2423, 0.1131, 0.0175],\n",
      "        [0.0230, 0.0874, 0.2764, 0.4087, 0.0000, 0.1265, 0.3356, 0.0000],\n",
      "        [0.2994, 0.0000, 0.2327, 0.4247, 0.0000, 0.0420, 0.0700, 0.0000],\n",
      "        [0.0000, 0.2328, 0.0786, 0.2625, 0.0000, 0.2241, 0.3045, 0.0000],\n",
      "        [0.0129, 0.1703, 0.0455, 0.1973, 0.0000, 0.2124, 0.0000, 0.2056],\n",
      "        [0.1077, 0.0080, 0.1167, 0.2033, 0.0000, 0.0057, 0.1286, 0.0000],\n",
      "        [0.0342, 0.0000, 0.0000, 0.0179, 0.0000, 0.0812, 0.0378, 0.0426],\n",
      "        [0.1575, 0.1370, 0.0081, 0.0344, 0.0000, 0.1163, 0.0167, 0.0000],\n",
      "        [0.0000, 0.1388, 0.0384, 0.1297, 0.0000, 0.1318, 0.1382, 0.0000],\n",
      "        [0.1267, 0.1609, 0.0066, 0.0000, 0.0000, 0.1114, 0.0000, 0.0000],\n",
      "        [0.0330, 0.0468, 0.0296, 0.0811, 0.0000, 0.0974, 0.0000, 0.0257],\n",
      "        [0.2163, 0.2726, 0.0000, 0.0000, 0.0000, 0.1720, 0.0203, 0.0000],\n",
      "        [0.0210, 0.1855, 0.0660, 0.1675, 0.0000, 0.1918, 0.0974, 0.0556],\n",
      "        [0.0000, 0.1369, 0.1647, 0.2506, 0.0000, 0.1143, 0.2745, 0.0000],\n",
      "        [0.1347, 0.0547, 0.0902, 0.0000, 0.0000, 0.1672, 0.0369, 0.0000],\n",
      "        [0.0349, 0.0000, 0.0459, 0.1019, 0.0000, 0.1130, 0.0000, 0.0753],\n",
      "        [0.1785, 0.0000, 0.1076, 0.2203, 0.0000, 0.0925, 0.0431, 0.0000],\n",
      "        [0.1618, 0.0000, 0.2084, 0.3478, 0.0000, 0.0016, 0.1761, 0.0000],\n",
      "        [0.0717, 0.0000, 0.0250, 0.0504, 0.0000, 0.0932, 0.0505, 0.0936],\n",
      "        [0.0000, 0.1251, 0.0000, 0.1171, 0.0307, 0.0370, 0.1034, 0.0000],\n",
      "        [0.1636, 0.2249, 0.0000, 0.0000, 0.0000, 0.1411, 0.0185, 0.0000],\n",
      "        [0.0173, 0.1128, 0.0349, 0.1389, 0.0000, 0.1570, 0.0000, 0.1236],\n",
      "        [0.1663, 0.0753, 0.0438, 0.0808, 0.0030, 0.0538, 0.0335, 0.0000],\n",
      "        [0.0865, 0.1186, 0.0000, 0.0000, 0.0000, 0.0636, 0.0098, 0.0000],\n",
      "        [0.2133, 0.0000, 0.1786, 0.3063, 0.0000, 0.0286, 0.0707, 0.0000],\n",
      "        [0.1868, 0.1839, 0.0123, 0.0009, 0.0511, 0.0940, 0.0047, 0.0000],\n",
      "        [0.0306, 0.1505, 0.0051, 0.1015, 0.0279, 0.0454, 0.0438, 0.0000],\n",
      "        [0.0000, 0.1758, 0.0549, 0.1943, 0.0000, 0.1718, 0.2228, 0.0000],\n",
      "        [0.1996, 0.0312, 0.0508, 0.1686, 0.0000, 0.1470, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1593, 0.1022, 0.1600, 0.0000, 0.1360, 0.2572, 0.0000],\n",
      "        [0.2722, 0.3200, 0.0177, 0.0037, 0.0000, 0.2658, 0.0045, 0.0000],\n",
      "        [0.2874, 0.3853, 0.0000, 0.0000, 0.0000, 0.2223, 0.0221, 0.0000],\n",
      "        [0.0245, 0.2666, 0.0938, 0.2521, 0.0000, 0.2698, 0.2192, 0.0087],\n",
      "        [0.0331, 0.2388, 0.0810, 0.2306, 0.0000, 0.2478, 0.0605, 0.1729],\n",
      "        [0.1464, 0.1590, 0.0600, 0.0000, 0.0000, 0.1504, 0.0265, 0.0000],\n",
      "        [0.1793, 0.2341, 0.0000, 0.0000, 0.0023, 0.1003, 0.0112, 0.0000],\n",
      "        [0.0225, 0.1212, 0.0384, 0.1282, 0.0000, 0.1353, 0.0442, 0.0410]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "print(result3)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0.0285, 0.1012, 0.0000, 0.2244, 0.0000, 0.0914, 0.1075, 0.0000],\n",
      "        [0.3209, 0.1905, 0.0000, 0.8378, 0.0000, 0.7812, 0.3445, 0.0000],\n",
      "        [0.1009, 0.6995, 0.0000, 0.2589, 0.0000, 0.9120, 0.0015, 0.0000],\n",
      "        [0.0280, 0.3540, 0.0000, 0.1658, 0.0000, 0.4240, 0.0000, 0.0000],\n",
      "        [0.2523, 0.3608, 0.0000, 0.4188, 0.0000, 0.6673, 0.2220, 0.0000],\n",
      "        [0.0386, 0.4898, 0.0000, 0.1965, 0.0000, 0.6452, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1196, 0.0000, 0.0808, 0.0000, 0.1416, 0.0000, 0.0000],\n",
      "        [0.2145, 0.0607, 0.0000, 0.5439, 0.0000, 0.3928, 0.2487, 0.0000],\n",
      "        [0.0000, 0.1787, 0.0000, 0.1597, 0.0000, 0.1607, 0.0234, 0.0777],\n",
      "        [0.2549, 0.0694, 0.0000, 0.6248, 0.0000, 0.5868, 0.2934, 0.0000],\n",
      "        [0.2426, 0.3294, 0.0000, 0.6245, 0.0000, 0.7101, 0.2497, 0.0000],\n",
      "        [0.0000, 0.1087, 0.0000, 0.1078, 0.0000, 0.0871, 0.0321, 0.0000],\n",
      "        [0.1571, 0.0745, 0.0000, 0.4531, 0.0000, 0.1725, 0.1131, 0.0000],\n",
      "        [0.1001, 0.5876, 0.0000, 0.2197, 0.0176, 0.7450, 0.0000, 0.0000],\n",
      "        [0.1672, 0.5474, 0.0000, 0.2422, 0.0000, 0.7306, 0.1126, 0.0000],\n",
      "        [0.0000, 0.3755, 0.0000, 0.1851, 0.0000, 0.4946, 0.0000, 0.0033],\n",
      "        [0.0099, 0.2483, 0.0000, 0.1076, 0.0000, 0.2463, 0.0000, 0.0000],\n",
      "        [0.0556, 0.3324, 0.0000, 0.1292, 0.0365, 0.4027, 0.0039, 0.0000],\n",
      "        [0.0000, 0.0693, 0.0000, 0.1190, 0.0000, 0.0312, 0.0390, 0.0000],\n",
      "        [0.1368, 0.1923, 0.0000, 0.3298, 0.0000, 0.3405, 0.1341, 0.0000],\n",
      "        [0.0111, 0.2178, 0.0000, 0.1159, 0.0000, 0.3025, 0.0000, 0.0516],\n",
      "        [0.0901, 0.0548, 0.0000, 0.3377, 0.0000, 0.2351, 0.1518, 0.0000],\n",
      "        [0.0000, 0.1073, 0.0000, 0.0924, 0.0000, 0.1008, 0.0000, 0.0000],\n",
      "        [0.2087, 0.1242, 0.0000, 0.5252, 0.0000, 0.4868, 0.2150, 0.0000],\n",
      "        [0.0148, 0.2236, 0.0000, 0.1115, 0.0000, 0.2663, 0.0000, 0.0000],\n",
      "        [0.0631, 0.4163, 0.0000, 0.1415, 0.0087, 0.5327, 0.0000, 0.0000],\n",
      "        [0.1174, 0.0473, 0.0000, 0.3721, 0.0000, 0.1790, 0.1266, 0.0000],\n",
      "        [0.0000, 0.0662, 0.0000, 0.0959, 0.0000, 0.0931, 0.0190, 0.0000],\n",
      "        [0.1418, 0.2581, 0.0000, 0.2306, 0.0000, 0.4168, 0.1422, 0.0000],\n",
      "        [0.0694, 0.4773, 0.0000, 0.1775, 0.0016, 0.6201, 0.0026, 0.0000],\n",
      "        [0.0150, 0.0694, 0.0000, 0.1517, 0.0000, 0.0595, 0.0731, 0.0000],\n",
      "        [0.0061, 0.2291, 0.0000, 0.0922, 0.0295, 0.2749, 0.0000, 0.0704],\n",
      "        [0.1866, 0.0569, 0.0000, 0.4495, 0.0000, 0.4101, 0.2157, 0.0000],\n",
      "        [0.0069, 0.1713, 0.0000, 0.0893, 0.0000, 0.1707, 0.0000, 0.0000],\n",
      "        [0.0410, 0.2044, 0.0000, 0.1740, 0.0000, 0.2144, 0.0546, 0.0000],\n",
      "        [0.0000, 0.1095, 0.0000, 0.2244, 0.0000, 0.1246, 0.0445, 0.0202],\n",
      "        [0.1132, 0.4100, 0.0000, 0.1685, 0.0000, 0.5386, 0.0783, 0.0000],\n",
      "        [0.0687, 0.1576, 0.0000, 0.2915, 0.0000, 0.2038, 0.0890, 0.0000],\n",
      "        [0.0000, 0.1829, 0.0000, 0.1341, 0.0000, 0.1855, 0.0078, 0.0932],\n",
      "        [0.0000, 0.2850, 0.0000, 0.1426, 0.0000, 0.3832, 0.0000, 0.0159],\n",
      "        [0.1839, 0.2817, 0.0000, 0.3898, 0.0000, 0.5127, 0.1837, 0.0000],\n",
      "        [0.0201, 0.3535, 0.0000, 0.1402, 0.0000, 0.4702, 0.0000, 0.0138],\n",
      "        [0.2789, 0.2603, 0.0000, 0.7467, 0.0000, 0.7392, 0.2808, 0.0000],\n",
      "        [0.2768, 0.1088, 0.0000, 0.7175, 0.0000, 0.6742, 0.3142, 0.0000],\n",
      "        [0.0054, 0.3360, 0.0000, 0.1638, 0.0000, 0.4350, 0.0000, 0.0000],\n",
      "        [0.0309, 0.2946, 0.0000, 0.1379, 0.0000, 0.3292, 0.0000, 0.0000],\n",
      "        [0.1528, 0.0486, 0.0000, 0.4285, 0.0000, 0.3177, 0.2061, 0.0000],\n",
      "        [0.1597, 0.0751, 0.0000, 0.3562, 0.0000, 0.3167, 0.1437, 0.0000],\n",
      "        [0.0059, 0.1533, 0.0000, 0.0898, 0.0000, 0.1850, 0.0000, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "interpreter": {
   "hash": "df0893f56f349688326838aaeea0de204df53a132722cbd565e54b24a8fec5f6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}